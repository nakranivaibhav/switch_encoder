{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pandas as pd\n",
    "from icecream import ic\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family: 'Bebas Neue'; font-size:1.2em;\">The wikitext dataset.\n",
    "\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:1em;\">Has 1.8 million rows of text from wikipedia\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:1em;\">Combined it to a very long string for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_parquet('train-00000-of-00002.parquet')\n",
    "df2 = pd.read_parquet('train-00001-of-00002.parquet')\n",
    "comb = pd.concat([df1,df2], ignore_index=True)\n",
    "\n",
    "text_data = \"\\n\".join(comb[\"text\"].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:24px;\">Tried training a custom tokenizer, but due to loading issues dropped it and grabbed a pre-trained tokenizer  \n",
    "<span style=\"font-family: 'Bebas Neue'; font-size:24px;\">Will eventually figure it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# tokenizer = BertWordPieceTokenizer(\n",
    "#     clean_text=True,\n",
    "#     handle_chinese_chars=True,\n",
    "#     strip_accents=True,\n",
    "#     lowercase=True,\n",
    "# )\n",
    "\n",
    "# tokenizer.train_from_iterator(\n",
    "#     text_data.split(\"\\n\"),\n",
    "#     vocab_size=32000,\n",
    "#     min_frequency=2,\n",
    "#     special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "#     limit_alphabet=1000,\n",
    "#     wordpieces_prefix=\"##\"\n",
    "# )\n",
    "\n",
    "# def tokenize_text(text_data, tokenizer, max_length, chunk_size):\n",
    "#     encodings = []\n",
    "#     for i in range(0, len(text_data), chunk_size):\n",
    "#         chunk = text_data[i:i+chunk_size]\n",
    "#         chunk_texts = chunk.split(\"\\n\")\n",
    "#         chunk_encodings = tokenizer.encode_batch(chunk_texts)\n",
    "#         for enc in chunk_encodings:\n",
    "#             encoding = {\n",
    "#                 \"ids\": enc.ids[:max_length],\n",
    "#                 \"attention_mask\": [1] * len(enc.ids[:max_length]),\n",
    "#             }\n",
    "#             overflow_ids = enc.ids[max_length:]\n",
    "#             while overflow_ids:\n",
    "#                 encoding[\"overflowing_tokens\"] = overflow_ids[:max_length]\n",
    "#                 encoding[\"overflow_attention_mask\"] = [1] * len(overflow_ids[:max_length])\n",
    "#                 overflow_ids = overflow_ids[max_length:]\n",
    "#                 encodings.append(encoding)\n",
    "#                 encoding = {\n",
    "#                     \"ids\": [],\n",
    "#                     \"attention_mask\": [],\n",
    "#                 }\n",
    "#     return encodings\n",
    "\n",
    "# # Set the chunk size according to your memory constraints\n",
    "# chunk_size = 10000  # Adjust this value based on your system's memory capacity\n",
    "\n",
    "# # Tokenize the text data in chunks\n",
    "# encodings = tokenize_text(text_data, tokenizer, max_length=256, chunk_size=chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:24px;\"> Will be working with 1% of the data for testing purposes  \n",
    "<span style=\"font-family: 'Bebas Neue'; font-size:24px;\"> Grabbed the next 0.5% of the data as valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = int(0.01 * len(text_data))\n",
    "train_text = text_data[:train_idx]\n",
    "test_text = text_data[train_idx:int(train_idx*1.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family: 'Bebas Neue'; font-size:34px;\">Gets the data ready for training.\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px;\">Grabbed a pre-trained tokenizer from HF. Since i will be closely following BERT, hence BERTTokenizer\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px;\">Makes a custom torch dataset \n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px;\">Data collator function from HF, which masks the text with prob of 0.15%.\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px;\">Operates on chunk of the text for faster processing\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px;\">Resultant train_dl and text_dl is obtained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "bs = 32\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class MaskedLanguageModelDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.encodings[idx][\"attention_mask\"],\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "def get_mlm(text_data):\n",
    "    def tokenize_text(text_data, tokenizer, max_length, chunk_size):\n",
    "        encodings = []\n",
    "        for i in range(0, len(text_data), chunk_size):\n",
    "            chunk = text_data[i:i+chunk_size]\n",
    "            chunk_encodings = tokenizer(chunk, truncation=True, max_length=max_length, return_overflowing_tokens=True, padding=True)\n",
    "            \n",
    "            for encoding in chunk_encodings.encodings:\n",
    "                encodings.append({\n",
    "                    \"input_ids\": encoding.ids,\n",
    "                    \"attention_mask\": encoding.attention_mask,\n",
    "                })\n",
    "        \n",
    "        return encodings\n",
    "\n",
    "    chunk_size = 10000  \n",
    "\n",
    "    encodings = tokenize_text(text_data, tokenizer, max_length=256, chunk_size=chunk_size)\n",
    "\n",
    "    dataset = MaskedLanguageModelDataset(encodings)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=bs,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "train_dl = get_mlm(train_text)\n",
    "test_dl = get_mlm(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px;\"> **input_ids**: Maps sub-words to inetgers according to the tokenizer\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px;\"> **attention_mask**: Ignores padded tokens\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px;\"> **labels**: Labesl for the task. -100 means ignore the label when calculating the loss in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,batch in enumerate(train_dl):\n",
    "    print(batch.input_ids[4][:20])\n",
    "    print(batch.attention_mask[4][:20])\n",
    "    print(batch.labels[4][:20])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family: 'Bebas Neue'; font-size:1.2em;\">Layer Norm\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:1.2em;\"> Two trainable params gamma and beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.param_shape = shape # 768\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(shape)) # 768\n",
    "        self.beta = nn.Parameter(torch.zeros(shape)) # 768\n",
    "        \n",
    "    def forward(self, x): # 32 x 256 x 768\n",
    "\n",
    "        dim = -1\n",
    "\n",
    "        mean = x.mean(dim = dim, keepdim=True) # Out shape: 30 x 256 x 1. Keepdim keeps the last dim and does not collapse.\n",
    "\n",
    "        var = ((x - mean)**2).mean(dim = dim,keepdim = True) # Out shape: 30 x 256 x 1. Keepdim keeps the last dim and does not collapse.\n",
    "\n",
    "        std = (var + self.eps).sqrt() # 30 x 256 x 1\n",
    "\n",
    "        y = (x - mean) / std # 30 x 256 x 768 Normalized values based on mean and std.\n",
    "\n",
    "        out = self.gamma * y + self.beta # learnable param same as embedding size. To incorporate lost information presumably or outliers/bias.\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family: 'Bebas Neue'; font-size:34px\">Embedding implementataion\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\"> Takes in vocab, gives embeddings for each token.\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\"> Position embeds are trainable, same as bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, vector_dim, max_len, drop):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, vector_dim) # 30552 x 768\n",
    "        self.position_embed = nn.Embedding(max_len, vector_dim) # 256 x 768\n",
    "        self.dropout = nn.Dropout(p = drop)\n",
    "        \n",
    "\n",
    "    def forward(self, x): # 32 x 256\n",
    "\n",
    "        token_embeddings = self.token_embeddings(x) # 32 x 256 x 768\n",
    "\n",
    "        seq_len = x.size(1) # Gets the seq len\n",
    "\n",
    "        position_ids = torch.arange(seq_len,dtype = torch.long,device = x.device) # Gives a tensor with [0,...,255]\n",
    "\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(x) # Unsqueeze adds a singleton dimension at pos 0. Then expanded with batch size same as x.\n",
    "\n",
    "        position_embeddings = self.position_embed(position_ids) # out: 32 x 256 x 768\n",
    "\n",
    "        embeddings = token_embeddings + position_embeddings # Add them\n",
    "\n",
    "        embeddings = self.dropout(embeddings) # Drop out\n",
    "\n",
    "        return embeddings # Out: 32 x 256 x 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:24px\"> Every word is broken down into three vectors q, k, v  \n",
    "\n",
    "<span style=\"font-family: 'Bebas Neue'; font-size:24px\"> Attention gives the probability of how much must we focus.  \n",
    "\n",
    "<span style=\"font-family: 'Bebas Neue'; font-size:24px\">The output is a vector with all the attention information incorporated...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdpa(q: torch.tensor, k: torch.tensor, v:torch.tensor, mask = None): # 32 x 8 x 256 x 96\n",
    "\n",
    "    d_k = q.shape[-1] # Takes the length of each head_dim i.e 64\n",
    "\n",
    "    #Selectively transposes the last two dimension of k for matmul. Gives attention scores for each word and every other word Out: 32 x 8 x 256 x 256\n",
    "    scaled = torch.matmul(q, k.transpose(-1,-2)) / math.sqrt(d_k) \n",
    "    \n",
    "    if mask is not None:\n",
    "        \n",
    "        mask_expanded = mask.unsqueeze(1).unsqueeze(2) # Add two dimension in the middle\n",
    "        \n",
    "        mask_expanded = mask_expanded.expand_as(scaled) # Expand as the scaled dimensions\n",
    "\n",
    "        scaled = scaled.masked_fill(mask_expanded == 0, float('-inf')) # Replace 0 values with - inf. Will not contribute to softmax -inf -> 0\n",
    "    \n",
    "\n",
    "    attention = F.softmax(scaled, dim = -1) # Squish the outputs, add up to one\n",
    "\n",
    "    out = torch.matmul(attention, v) #The transformed vector with attention scores incorporated. Out: (32, 8, 256, 96) \n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:34px\">Multi-Head Attention\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\"> Split across many heads for parallel comuptation.\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\"> The intutuion is many heads can learn better attention scores than a single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,vector_dim,num_heads,drop):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.vector_dim = vector_dim # 768\n",
    "        self.num_heads = num_heads # 8\n",
    "        self.head_dims = vector_dim // num_heads # 768 / 8 = 96\n",
    "        self.qkv_layer = nn.Linear(vector_dim, vector_dim * 3) # shape: (768, 2304)\n",
    "        self.linear = nn.Linear(vector_dim, vector_dim) #shape: (768, 768)\n",
    "        self.dropout = nn.Dropout(p = drop)\n",
    "        self.layer_norm = LayerNormalization(vector_dim)\n",
    "\n",
    "    def forward(self, x, mask = None): # 32 x 256 x 768\n",
    "\n",
    "        bs, max_len, _ = x.shape # Gets the batch size and max length\n",
    "\n",
    "        qkv = self.qkv_layer(x) # Splits the inp embedding into three diff values but together for faster speeds. Out: 32 x 256 x 2304\n",
    "\n",
    "        qkv = qkv.reshape(bs, max_len, self.num_heads, self.head_dims * 3) # Divide according to the number of heads. Out: 32 x 256 x 8 x 288\n",
    "\n",
    "        qkv = qkv.permute(0,2,1,3) # Reshaping to get it ready for inp into attention head. Out: 32 x 8 x 256 x 288\n",
    "\n",
    "        q, k, v = qkv.chunk(3,dim = -1) # Chunk it into 3 parts Q, K and V Out: (32, 8, 256, 96)\n",
    "\n",
    "        out = sdpa(q, k, v, mask) # Scaled dot product attention Out: (32, 8, 256, 96)\n",
    "\n",
    "        out = out.reshape(bs, max_len, self.num_heads * self.head_dims) # Back to vector_size with attention info incorporated Out: (32, 256, 768)\n",
    "\n",
    "        out = self.dropout(out) # Drop out\n",
    "\n",
    "        out = self.linear(out) # Final Linear transformation Out: Out: (32, 256, 768)\n",
    "\n",
    "        out = self.layer_norm(out) # Normalization\n",
    "        \n",
    "        out = self.dropout(out) # Drop out\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:34px\"> Feed Forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosFFN(nn.Module):\n",
    "    def __init__(self, vector_dim, hidden_dim, drop):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(vector_dim,hidden_dim) # 768 x 2048\n",
    "        self.lin2 = nn.Linear(hidden_dim,vector_dim) # 2048 x 768\n",
    "        self.gelu = nn.GELU()\n",
    "        self.drop = nn.Dropout(p=drop)\n",
    "        self.layer_norm = LayerNormalization(hidden_dim)\n",
    "\n",
    "    def forward(self, x): # 32 x 256 x 768\n",
    "\n",
    "        x = self.lin1(x) # Out: 32 x 256 x 2048\n",
    "\n",
    "        x = self.gelu(x) # Act Out: 32 x 256 x 2048\n",
    "\n",
    "        x = self.layer_norm(x) # Out: 32 x 256 x 2048\n",
    "\n",
    "        x = self.drop(x) # Dropout Out: 32 x 256 x 2048\n",
    "\n",
    "        x = self.lin2(x) #Linear projection to vector_dim. Out:32 x 256 x 768\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:34px\"> Encoder block\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\"> The Multi-head attention and FFN to get a single encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,vector_dim,num_heads,hidden_dim,drop):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(vector_dim,num_heads,drop)\n",
    "        self.ffn = PosFFN(vector_dim = vector_dim,hidden_dim = hidden_dim,drop = drop)\n",
    "    \n",
    "    def forward(self, x, attention_mask): # 32 x 256 x 768\n",
    "        res_x = x # Residual Out: 32 x 256 x 768\n",
    "\n",
    "        x = self.attention(x,mask = attention_mask) # Out: 32 x 256 x 768\n",
    "\n",
    "        x = x + res_x # Add the residual\n",
    "\n",
    "        res_x = x # Take the residual again Out: 32 x 256 x 768\n",
    "\n",
    "        x = self.ffn(x) # Position wise feed forward Out: 32 x 256 x 768\n",
    "\n",
    "        x = x + res_x # Add the residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:34px\"> Stacking the encoder blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vector_dim,num_heads,drop,hidden_dim,num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderBlock(vector_dim,num_heads,hidden_dim,drop) for _ in range(num_layers)])\n",
    "    def forward(self, x, attention_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:34px\"> The final model.\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\">Ready for token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dim = 768\n",
    "num_heads = 8\n",
    "drop = 0.1\n",
    "max_len = 256\n",
    "hidden_dim = 2048\n",
    "num_layers = 2\n",
    "vocab_size = len(tokenizer.get_vocab()) # 30552\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, vector_dim, max_len, num_heads, drop, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, vector_dim, max_len, drop)\n",
    "        self.encoder = Encoder(vector_dim, num_heads, drop, hidden_dim, num_layers)\n",
    "        self.linear = nn.Linear(vector_dim, vocab_size)  # Add a linear layer for output projection\n",
    "    \n",
    "    def forward(self, x, attention_mask):\n",
    "        x = self.embeddings(x) # Out: 32 x 256 x 768\n",
    "        \n",
    "        x = self.encoder(x, attention_mask) # Out: 32 x 256 x 768\n",
    "\n",
    "        x = self.linear(x)  # Project the output to the vocabulary size\n",
    "        \n",
    "        return x\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "bs = 32\n",
    "input_tensor = torch.randint(0, vocab_size, (bs, max_len))\n",
    "attention_mask = torch.ones(bs, max_len)  # Create an attention mask with all ones\n",
    "\n",
    "model = TransformerModel(vocab_size, vector_dim, max_len, num_heads, drop, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# Generate a random input tensor and attention mask\n",
    "bs = 32\n",
    "input_tensor = torch.randint(0, vocab_size, (bs, max_len)).to(device)\n",
    "attention_mask = torch.ones(bs, max_len).to(device) # Create an attention mask with all ones\n",
    "\n",
    "# Pass the input tensor and attention mask through the model\n",
    "output = model(input_tensor, attention_mask)\n",
    "\n",
    "# Print the shapes of the input and output tensors\n",
    "print(\"Input tensor shape:\", input_tensor.shape)\n",
    "print(\"Attention mask shape:\", attention_mask.shape)\n",
    "print(\"Output tensor shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dim = 768\n",
    "num_heads = 8\n",
    "drop = 0.1\n",
    "max_len = 256\n",
    "hidden_dim = 2048\n",
    "num_layers = 2\n",
    "vocab_size = len(tokenizer.get_vocab()) # 30552\n",
    "\n",
    "from tqdm import tqdm\n",
    "model = TransformerModel(vocab_size, vector_dim, max_len, num_heads, drop, hidden_dim, num_layers)\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_dl, desc=f'Epoch {epoch+1}')\n",
    "    model.train()\n",
    "    for batch in pbar:\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask).to(device)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    print(f'Train Loss: {train_epoch_loss}')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar2 = tqdm(test_dl, desc=f'Epoch {epoch + 1}')\n",
    "        \n",
    "        for batch in pbar2:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask).to(device)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "    train_epoch_loss = train_loss / len(train_dl)\n",
    "    valid_loss = val_loss / len(test_dl)\n",
    "    print(f'Valid Loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:24px\"> WIP: Attention mechanism for the routing gate\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\"> Trying it with 1 attention head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SingleHeadRouterAttention(nn.Module):\n",
    "#     def __init__(self,vector_dim,num_experts,num_heads = 1,bs=32,max_len=256):\n",
    "#         super().__init__()\n",
    "#         self.vector_dim = vector_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dims = vector_dim // num_heads # 768\n",
    "#         self.qkv_layer = nn.Linear(vector_dim, vector_dim * 3) # shape: (768, 2304)\n",
    "#         self.linear = nn.Linear(vector_dim,num_experts) #shape: (768, num_experts)\n",
    "#         self.bs = bs\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#     def forward(self, x, mask = None): # 32 x 256 x 768\n",
    "        \n",
    "#         bs, max_len, _ = x.shape # Gets the batch size and max length\n",
    "\n",
    "#         qkv = self.qkv_layer(x) # Splits the inp embedding into three diff values but together for faster speeds. Out: 32 x 256 x 2304\n",
    "\n",
    "#         qkv = qkv.reshape(bs, max_len, self.num_heads, self.head_dims * 3) # Divide according to the number of heads. Out: 32 x 256 x 8 x 288\n",
    "\n",
    "#         qkv = qkv.permute(0,2,1,3) # Reshaping to get it ready for inp into attention head. Out: 32 x 8 x 256 x 288\n",
    "\n",
    "#         q, k, v = qkv.chunk(3,dim = -1) # Chunk it into 3 parts Q, K and V Out: (32, 8, 256, 96)\n",
    "\n",
    "#         out = sdpa(q, k, v, mask) # Scaled dot product attention Out: (32, 8, 256, 96)\n",
    "\n",
    "#         out = out.reshape(bs, max_len, self.num_heads * self.head_dims) # Back to vector_size with attention info incorporated Out: (32, 256, 768)\n",
    "\n",
    "#         out = self.linear(out) # Final Linear transformation Out: Out: (32, 256, 768)\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:34px\">Sparse Feed Forward Network\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\"> Takes in number of experts and top_k, the top k experts to route the tokens.\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\"> Implements a linear gating mechanism \n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\"> The output is multiplied by the routing weights, appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchFFN(nn.Module):\n",
    "    def __init__(self, vector_dim, hidden_dim, num_experts, expert_drop = 0.4,top_k = 1,capacity_factor = 1.25):\n",
    "        super().__init__()\n",
    "        self.vector_dim = vector_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = int(top_k)\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.gate = nn.Linear(self.vector_dim, self.num_experts, bias=False) # 768 x 4\n",
    "        self.experts = nn.ModuleList([PosFFN(vector_dim,hidden_dim,expert_drop) for _ in range(self.num_experts)]) #out shape: (32, 256, 768)\n",
    "\n",
    "    def forward(self, x): # 32 x 256 x 768\n",
    "        \n",
    "        batch_size, sequence_length, vector_dim = x.shape \n",
    "\n",
    "        x = x.view(-1, vector_dim) #Flattens to num_tokens [8192] -> 32 X 256, vector_dim -> 768\n",
    "\n",
    "        router_logits = self.gate(x) #Passes it through the routing gate. In shape: 8192 x 768 Out shape: 8192 x 4 -> Router logits for each token\n",
    "        \n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float) # Probabilities of routing\n",
    "        \n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1) # Gives 2 tensors. 1 > routing weights 2 > The expert index\n",
    "\n",
    "        routing_weights /= routing_weights.sum(dim=-1, keepdim=True) # Scales the routing scores according to the experts\n",
    "        \n",
    "        routing_weights = routing_weights.to(x.dtype) # Cast it back to fp16. Unfortunately i have mps, i have no fp16\n",
    "\n",
    "        #initializes the intermediate output to all zeros. Need this to add out in specific indexes\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, vector_dim), dtype=x.dtype, device=x.device\n",
    "        )\n",
    "\n",
    "        # Calculate expert capacity\n",
    "        tokens_per_batch = batch_size * sequence_length\n",
    "        expert_capacity = int(tokens_per_batch / self.num_experts) * self.capacity_factor\n",
    "\n",
    "        #Shape: 4,1,8192. A one hot encoded mask for each token num_experts x top_k x tokens\n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0) \n",
    "        \n",
    "        # Looping through the all the experts and perform comp on each expert.\n",
    "        for expert_idx in range(self.num_experts):\n",
    "\n",
    "            expert_layer = self.experts[expert_idx] # Gets the expert layer\n",
    "\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx]) # \n",
    "\n",
    "            #If no experts are assigned, the loop contunues to the next expert\n",
    "            if top_x.shape[0] == 0:\n",
    "                continue\n",
    "            if top_x.shape[0] > expert_capacity:\n",
    "                leftover_tokens = top_x[expert_capacity:]\n",
    "                top_x = top_x[:expert_capacity]\n",
    "            \n",
    "            current_state = x[None, top_x].reshape(-1, vector_dim) # In: 8192 x 768\n",
    "\n",
    "            #Indexes into the routing weights and multiplies with out from layer\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "\n",
    "            final_hidden_states[top_x] = current_hidden_states.to(x.dtype)\n",
    "\n",
    "            if leftover_tokens.shape[0] > 0:\n",
    "                final_hidden_states[leftover_tokens] = x[None, leftover_tokens].reshape(-1, vector_dim)\n",
    "        \n",
    "\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, vector_dim) #reshapes\n",
    "        \n",
    "        return final_hidden_states, router_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:34px\">Switch Encoder\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\"> Inserts a switch layer after three normal FFN layers\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\"> Takes the router logits if switch layer. (required to calculate the load balancing loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchEncoderBlock(nn.Module):\n",
    "    def __init__(self, vector_dim, num_heads, hidden_dim, num_layers, num_experts,top_k = 1,expert_drop = 0.4,drop = 0.1):  # Added MoE parameters\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(vector_dim, num_heads, drop)\n",
    "        self.layers = nn.Sequential(*[\n",
    "            PosFFN(vector_dim, hidden_dim, drop) if i % 4 != 3  # Insert EncoderBlock at most positions\n",
    "            else SwitchFFN(vector_dim, hidden_dim, num_experts, top_k, expert_drop)  # Insert SparseMoEBlock every 4th layer\n",
    "            for i in range(num_layers) \n",
    "        ])\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        total_router_logits = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, SwitchFFN):\n",
    "                #Attention\n",
    "                residual_x = x\n",
    "                x = self.attention(x, attention_mask)\n",
    "                x = residual_x + x\n",
    "                residual_x = x\n",
    "                \n",
    "                x, router_logits = layer(x)  # Collect from MoE\n",
    "                x = residual_x + x\n",
    "                total_router_logits.append(router_logits)\n",
    "            else:\n",
    "                residual_x = x\n",
    "\n",
    "                x = self.attention(x, attention_mask)\n",
    "\n",
    "                x = residual_x + x\n",
    "\n",
    "                residual_x = x\n",
    "\n",
    "                x = layer(x)\n",
    "\n",
    "                x = residual_x + x\n",
    "        total_router_logits = torch.cat(total_router_logits, dim=0)\n",
    "        return x, total_router_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:34px\">The Final Switch Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dim = 768\n",
    "num_heads = 8\n",
    "drop = 0.1\n",
    "max_len = 256\n",
    "hidden_dim = 2048\n",
    "num_layers = 8\n",
    "num_experts = 4\n",
    "expert_drop = 0.4\n",
    "top_k = 1\n",
    "vocab_size = len(tokenizer.get_vocab()) # 30552\n",
    "\n",
    "class SwitchTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, vector_dim, max_len, num_heads, drop, hidden_dim, num_layers,top_k = 1, expert_drop = 0.4):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, vector_dim, max_len, drop)\n",
    "        self.encoder = SwitchEncoderBlock(vector_dim, num_heads, hidden_dim, num_layers, num_experts,top_k,expert_drop) #Initialize the switch encoder\n",
    "        self.linear = nn.Linear(vector_dim, vocab_size)  # Add a linear layer for output projection\n",
    "    \n",
    "    def forward(self, x, attention_mask):\n",
    "        x = self.embeddings(x) # Out: 32 x 256 x 768\n",
    "        \n",
    "        x, router_logits = self.encoder(x, attention_mask) # Out: 32 x 256 x 768\n",
    "\n",
    "        x = self.linear(x)  # Project the output to the vocabulary size\n",
    "        \n",
    "        return x, router_logits\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "bs = 32\n",
    "input_tensor = torch.randint(0, vocab_size, (bs, max_len))\n",
    "attention_mask = torch.ones(bs, max_len)  # Create an attention mask with all ones\n",
    "\n",
    "model = SwitchTransformer(vocab_size, vector_dim, max_len, num_heads, drop, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# Generate a random input tensor and attention mask\n",
    "bs = 32\n",
    "input_tensor = torch.randint(0, vocab_size, (bs, max_len)).to(device)\n",
    "attention_mask = torch.ones(bs, max_len).to(device) # Create an attention mask with all ones\n",
    "\n",
    "# Pass the input tensor and attention mask through the model\n",
    "output,router_logits = model(input_tensor, attention_mask)\n",
    "\n",
    "# Print the shapes of the input and output tensors\n",
    "print(\"Input tensor shape:\", input_tensor.shape)\n",
    "print(\"Attention mask shape:\", attention_mask.shape)\n",
    "print(\"Output tensor shape:\", output.shape)\n",
    "print(\"Router Logits shape:\", router_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Bebas Neue'; font-size:34px\">Load balancing loss\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\">Calculates token processed per expert.\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\">Route percentages per expert\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\">Mean and Sum both of them\n",
    "- <span style=\"font-family: 'Bebas Neue'; font-size:24px\">Scales by a hyper param alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_loss(router_logits,num_experts = num_experts,top_k = top_k,attention_mask = None,alpha=10e-2):\n",
    "    \"\"\"\n",
    "    inp: router logits from the sparse layers, top_k experts no., attention_mask, tunable hyper-param to scale the loss\n",
    "    \"\"\"\n",
    "   \n",
    "    # Apply softmax to router_logits to get the probabilities\n",
    "    router_probs = F.softmax(router_logits, dim=1)\n",
    "\n",
    "    _, selected_experts = torch.topk(router_probs, top_k, dim=-1) # Gives selected ecpdrt for each token: Shape: (8192,1)\n",
    "\n",
    "    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)# OHE mask for each token Out: (8192, 1, 4)\n",
    "\n",
    "    tok_per_expert = torch.mean(expert_mask.float(), dim=0) # Gives token fraction per expert i Out: (1, 4)\n",
    "\n",
    "    rout_per_expert = torch.mean(router_probs, dim=0) # Gives fraction of router prob allotted to expert i Out: (4)\n",
    "\n",
    "    loss = torch.sum(tok_per_expert * rout_per_expert.unsqueeze(0))\n",
    "    # Scale the loss by alpha\n",
    "    loss = alpha * loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SwitchTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwitchTransformer(vocab_size, vector_dim, max_len, num_heads, drop, hidden_dim, num_layers)\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_dl, desc=f'Epoch {epoch+1}')\n",
    "    model.train()\n",
    "    for batch in pbar:\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs,router_logits = model(input_ids, attention_mask)\n",
    "        outputs.to(device),router_logits.to(device)\n",
    "\n",
    "        #Aggregrate Loss\n",
    "        load_balance_loss = load_loss(router_logits)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "        loss += load_balance_loss\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix(loss=train_loss / (pbar.n + 1))\n",
    "        \n",
    "    print(f'Train_loss:{train_loss/len(train_dl)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
